{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut0G-fB-wKw0",
        "outputId": "b4fe94df-fa33-42b5-c0f1-5177d9bfdfa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Using cached lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.6)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.1+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Using cached torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Using cached pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.2.106)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.1.0->lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.1.0->lightning) (12.6.20)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n",
            "Using cached lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "Using cached pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "Installing collected packages: nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.4.0 nvidia-cudnn-cu12-8.9.2.26 nvidia-cusolver-cu12-11.4.5.107 pytorch-lightning-2.4.0 torchmetrics-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import numpy as np\n",
        "import scipy.ndimage as ndi\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "#imports for pyroch lighting\n",
        "import lightning as L\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z_YGioLAwM6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbeca8d2-e3e8-4724-9138-cf3787438fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of CNN-based Variatonal Autoencoder Model with Pytorch Lighting"
      ],
      "metadata": {
        "id": "60SLUicWcCWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder implementation\n",
        "#can use 4th conv layer in encoder and 1st transpose_conv layer in decoder for higher compression. Currently not used in forward functions\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):  # Adjust latent_dim as needed\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)  # 256 -> 128\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)  # 128 -> 64\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)  # 64 -> 32\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # 32 -> 16\n",
        "\n",
        "        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)  # Latent mean\n",
        "        self.fc_log_var = nn.Linear(256 * 16 * 16, latent_dim)  # Latent log-variance\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten for linear layer\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_log_var(x)\n",
        "        return mu, log_var\n",
        "\n",
        "#VAE Decoder Implementation\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 256 * 16 * 16) #flattened, so need to match the size\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)  # 16 -> 32\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)  # 32 -> 64\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)  # 64 -> 128\n",
        "        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)  # 128 -> 256\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(x.size(0), 256, 16, 16) #reshape back to prev conv layer's output shape\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = F.relu(self.deconv3(x))\n",
        "        x = torch.sigmoid(self.deconv4(x)) #Sigmoid for pixel values in [0, 1]\n",
        "        return x"
      ],
      "metadata": {
        "id": "zf70hXLxXtu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reparam step for VAEs:\n",
        "def reparameterize(mu, log_var):\n",
        "    std = torch.exp(0.5 * log_var)\n",
        "    epsilon = torch.randn_like(std)\n",
        "    return mu + std * epsilon"
      ],
      "metadata": {
        "id": "cWHnRpHEYKmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VAE Class\n",
        "class VAE(L.LightningModule):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.test_loss_list = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = reparameterize(mu, log_var)\n",
        "        reconstructed_img = self.decoder(z)\n",
        "        return reconstructed_img, mu, log_var #after one encoder-decoder pass, recon_img used to calculate recon_loss. (mu, log_var) for kl_divergence\n",
        "\n",
        "    def _get_vae_loss(self, batch):\n",
        "        x = batch\n",
        "        x_hat, mu, log_var = self(x) #get from forward pass\n",
        "\n",
        "        #recon loss\n",
        "        recon_loss = F.mse_loss(x_hat, x, reduction='sum')\n",
        "\n",
        "        #KL divergence loss\n",
        "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "        return recon_loss + kl_loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._get_vae_loss(batch) #minimize on combined recon, kl loss\n",
        "        self.log(\"train_loss\", loss, logger=True, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._get_vae_loss(batch)\n",
        "        self.log(\"val_loss\", loss, logger=True, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self._get_vae_loss(batch)\n",
        "        self.test_loss_list.append(loss.item())\n",
        "        self.log(\"test_loss\", loss, logger=True, prog_bar=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self): #return the losses of test samples. Accssed via class-object.test_loss_list\n",
        "        return {\"test_loss_list\": self.test_loss_list}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #add l2 reg term to reduce overfitting (weight_decay)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        #lr scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=10, min_lr=5e-5)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n"
      ],
      "metadata": {
        "id": "dMwJwANnYQI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VAE Initializations\n",
        "latent_dim = 128 #latent dimension for the latent space of VAE. Larger might be better for larger images\n",
        "\n",
        "encoder = Encoder(latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim)\n",
        "\n",
        "vae = VAE(encoder, decoder)"
      ],
      "metadata": {
        "id": "7gxIU6qdY07J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset implementation"
      ],
      "metadata": {
        "id": "vCi_FmWtk5AR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing VAE with Mnist and Mnist fashion datset to see if model is working correctly. Fahsion dataset is anomly data. If working correctly, the Fashion Dataset should have higher loss than MNIST digits"
      ],
      "metadata": {
        "id": "07pxDlvoNUwa"
      }
    }
  ]
}